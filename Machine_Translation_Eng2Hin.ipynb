{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation: Eng2Hin.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TaC5moIwclS"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import io\n",
        "import time\n",
        "import warnings\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hq-9B_LdHvS"
      },
      "source": [
        "# Prepare the data\n",
        "1. Convert the texts into a DataFrame with English and Hindi columns\n",
        "2. Storing it Google-Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agAikoo7w8OF"
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "        # open the file\n",
        "        file = open(filename, mode='rt', encoding='utf-8')\n",
        "        \n",
        "        # read all text\n",
        "        text = file.read()\n",
        "        file.close()\n",
        "        return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6khg00Wy78a"
      },
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "      sents = text.strip().split('\\n')\n",
        "      sents = [i.split('\\t') for i in sents]\n",
        "      return sents"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0M-5zX4FEXe_",
        "outputId": "fba38215-aa4a-4d24-9391-3a1a46178b9b"
      },
      "source": [
        "# mount the G-Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuccTTm2zBDs"
      },
      "source": [
        "# read the files\n",
        "en = read_text('/content/drive/MyDrive/kroop_data/train01.en') # path for english texts\n",
        "en = to_lines(en) \n",
        "hd = read_text('/content/drive/MyDrive/kroop_data/train01.hi') # path for hindi texts\n",
        "hd = to_lines(hd)\n",
        "\n",
        "# covert the text files into DataFrame\n",
        "hd_df = pd.DataFrame(hd, columns = ['hd'])\n",
        "en_df = pd.DataFrame(en, columns = ['en'])\n",
        "df = pd.concat([en_df, hd_df], axis = 1)\n",
        "\n",
        "# save the DataFrame as csv \n",
        "df.to_csv('data.csv')\n",
        "PATH = '/content/data.csv'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9srWW9fdB2B"
      },
      "source": [
        "# Preprocess English and Hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFWjcIF3xrMD"
      },
      "source": [
        "# converts unicode to ASCII\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# preprocess english sentence\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "# preprocess hindi sentence\n",
        "def hindi_preprocess_sentence(w):\n",
        "    w = w.rstrip().strip()\n",
        "    return w\n",
        "\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM-_YKbCy-qf"
      },
      "source": [
        "# clean the data\n",
        "def create_dataset(path=PATH):\n",
        "    lines=pd.read_csv(path,encoding='utf-8')\n",
        "    lines=lines.dropna()\n",
        "    en = []\n",
        "    hd = []\n",
        "    for i, j in zip(lines['en'], lines['hd']):\n",
        "        en_1 = [preprocess_sentence(w) for w in i.split(' ')]\n",
        "        en_1.append('<end>')\n",
        "        en_1.insert(0, '<start>')\n",
        "        hd_1 = [hindi_preprocess_sentence(w) for w in j.split(' ')]\n",
        "        hd_1.append('<end>')\n",
        "        hd_1.insert(0, '<start>')\n",
        "        en.append(en_1)\n",
        "        hd.append(hd_1)\n",
        "    return hd, en"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czmql4XB8_m0"
      },
      "source": [
        "# get the maximum length tensor\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP5Np-CGdxAa"
      },
      "source": [
        "# Tokenization of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKMJyk7H9D1z"
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-EMa-8H9KIx"
      },
      "source": [
        "# load the dataset in the required format\n",
        "def load_dataset(path=PATH):\n",
        "    targ_lang, inp_lang = create_dataset(path)\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K5wC3lm7pDI"
      },
      "source": [
        "# use the above functions\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(PATH)\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNxjL-PAd80j"
      },
      "source": [
        "# Create Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xDkgJUu1hkP",
        "outputId": "2263bb61-db41-45e3-adf9-59e34778f553"
      },
      "source": [
        "# split the train and validation set\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1912 1912 478 478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75--UOLg5aC-",
        "outputId": "d44c2d36-804b-47b5-d5ef-7d00f0c6da26"
      },
      "source": [
        "# check the tokenized words\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "2 ----> <start>\n",
            "25 ----> we\n",
            "1080 ----> asserted\n",
            "19 ----> our\n",
            "5310 ----> supremacy\n",
            "7 ----> in\n",
            "1 ----> the\n",
            "59 ----> world\n",
            "29 ----> by\n",
            "5311 ----> introducing\n",
            "5312 ----> tejas\n",
            "5313 ----> aircraft .\n",
            "3 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "31 ----> हम\n",
            "5307 ----> ‘तेजस’\n",
            "584 ----> हवाई\n",
            "5308 ----> जहाज\n",
            "3 ----> के\n",
            "74 ----> द्वारा\n",
            "40 ----> आज\n",
            "103 ----> दुनिया\n",
            "3 ----> के\n",
            "1163 ----> अंदर\n",
            "73 ----> अपनी\n",
            "2338 ----> अहमियत\n",
            "1164 ----> पहुंचा\n",
            "47 ----> रहे\n",
            "21 ----> हैं।\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTKPfvyeeRIT"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHob_bku2dNG"
      },
      "source": [
        "# define parameters\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2vkStI3eeJ5"
      },
      "source": [
        "# Encoder Decoder with Attention Model\n",
        "Here we use a sequence to sequence model which has two parts – an encoder and a decoder. Both the parts have two different neural network models combined into one giant network. The input is put through an encoder model which gives us the encoder output. Here, each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOxaToqGei26"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUiDvRuX2zBP"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqNMZYjdeo5s"
      },
      "source": [
        "Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuPNEMd3-mwN"
      },
      "source": [
        "class Attention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77no1nmMeqzJ"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lILngiqU-st1"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.attention = Attention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    x = self.embedding(x)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    x = self.fc(output)\n",
        "    return x, state, attention_weights\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIVIywSoewak"
      },
      "source": [
        "Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP0TXtl4-wpQ"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "# get the mean loss\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM4g48dF-1f6"
      },
      "source": [
        "# define the checkpoint\n",
        "checkpoint_dir = '/content/drive/MyDrive/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Q6vRXXUe1dg"
      },
      "source": [
        "# Training\n",
        "\n",
        "1. Pass input through encoder to get encoder output.\n",
        "2. Then encoder output, encoder hidden state and the decoder input is passed to decoder.\n",
        "3. Decoder returns predictions and decoder hidden state.\n",
        "4. Decoder hidden state is then passed back to model.\n",
        "5. Predictions are used to calculate loss.\n",
        "6. Use teacher forcing (technique where the target word is passed as the next input) for the next input to the decoder.\n",
        "7. Calculate gradients and apply it to optimizer for backpropogation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_2Hxc_MZDYL"
      },
      "source": [
        "# steps to be followed while training\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))      \n",
        "  return batch_loss"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTuuk8km_BeJ",
        "outputId": "e56e92fc-72aa-4b08-ea8a-497c8f9b87b1"
      },
      "source": [
        "# training\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0557\n",
            "Epoch 1 Batch 100 Loss 1.3809\n",
            "Epoch 1 Loss 1.6452\n",
            "Time taken for 1 epoch 338.72101354599 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.3608\n",
            "Epoch 2 Batch 100 Loss 1.5045\n",
            "Epoch 2 Loss 1.4912\n",
            "Time taken for 1 epoch 262.8338351249695 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.6090\n",
            "Epoch 3 Batch 100 Loss 1.4806\n",
            "Epoch 3 Loss 1.4352\n",
            "Time taken for 1 epoch 257.74615454673767 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2369\n",
            "Epoch 4 Batch 100 Loss 1.3371\n",
            "Epoch 4 Loss 1.3864\n",
            "Time taken for 1 epoch 266.54276061058044 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1854\n",
            "Epoch 5 Batch 100 Loss 1.5004\n",
            "Epoch 5 Loss 1.3418\n",
            "Time taken for 1 epoch 260.319144487381 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.3573\n",
            "Epoch 6 Batch 100 Loss 1.1337\n",
            "Epoch 6 Loss 1.2953\n",
            "Time taken for 1 epoch 260.05254340171814 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4602\n",
            "Epoch 7 Batch 100 Loss 1.1731\n",
            "Epoch 7 Loss 1.2474\n",
            "Time taken for 1 epoch 260.34270095825195 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0988\n",
            "Epoch 8 Batch 100 Loss 0.9545\n",
            "Epoch 8 Loss 1.1966\n",
            "Time taken for 1 epoch 261.8207848072052 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.2098\n",
            "Epoch 9 Batch 100 Loss 1.0200\n",
            "Epoch 9 Loss 1.1426\n",
            "Time taken for 1 epoch 261.3980474472046 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.1026\n",
            "Epoch 10 Batch 100 Loss 1.1578\n",
            "Epoch 10 Loss 1.0931\n",
            "Time taken for 1 epoch 262.51372265815735 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAq46GdFfhk6"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0fq1Gkj_F4Y"
      },
      "source": [
        "# predict the translated sentence\n",
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpcJ-spKnbMY"
      },
      "source": [
        "# to print the translated sentence\n",
        "def translate(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRAnO1LmnfL6",
        "outputId": "81c35e44-786a-423c-a7ed-a5ccd382e0c4"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9eeb39cb10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrBnMVpY-_f-"
      },
      "source": [
        "Examples for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1Bdzshdx30u",
        "outputId": "7b11529e-4204-46a1-a21e-e6213e8107d1"
      },
      "source": [
        "translate(u'prime minister has a scheme for women')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: prime minister has a scheme for women\n",
            "Predicted translation: प्रधानमंत्री ने कहा कि भारत और एक बात है। <end> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5UNKUVsx34A",
        "outputId": "2e83c7b1-7f76-4aed-a811-6d02a3cf7b65"
      },
      "source": [
        "translate(u'When did India get independent')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: when did india get independent\n",
            "Predicted translation: मैं भारत के लिए एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CDQUJAhx3ya",
        "outputId": "01ee05b5-a5e0-47ac-df2c-11fb474675e6"
      },
      "source": [
        "translate(u'Prime Minister addressed a public gathering')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: prime minister addressed a public gathering\n",
            "Predicted translation: प्रधानमंत्री ने कहा कि भारत के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने के लिए एक बात करने \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C5mgFduni6u",
        "outputId": "9b807498-45f5-4c73-9254-b49079848828"
      },
      "source": [
        "translate(u'India is a democratic country')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: india is a democratic country\n",
            "Predicted translation: मैं भारत में भी एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZDAb0hpx4IW",
        "outputId": "a8cf0e0c-4fc0-48e4-b38b-988104ef9f03"
      },
      "source": [
        "translate(u'India is a democratic country')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: india is a democratic country\n",
            "Predicted translation: मैं भारत में भी एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार एक बार \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}